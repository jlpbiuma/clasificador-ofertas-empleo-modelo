{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_5 (LSTM)               (None, 64)                16896     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17026 (66.51 KB)\n",
      "Trainable params: 17026 (66.51 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6946 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6929 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6911 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6893 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6876 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6859 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6841 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6824 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6806 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6789 - accuracy: 0.7500\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd0fab75160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "Predictions for Registers: [[0.49557513 0.50442487]\n",
      " [0.4863222  0.51367784]\n",
      " [0.50169927 0.4983008 ]\n",
      " [0.47824523 0.5217548 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the unique properties and create a vocabulary\n",
    "unique_properties = ['A', 'B', 'C', 'D']\n",
    "property_to_index = {prop: idx for idx, prop in enumerate(unique_properties)}\n",
    "\n",
    "# Sample registers and their corresponding labels\n",
    "registers = [\n",
    "    ['A', 'B', 'C'],\n",
    "    ['A', 'C', 'D'],\n",
    "    ['B', 'C', 'E'],\n",
    "    ['A', 'B', 'C', 'D'],\n",
    "    # Add more registers as needed\n",
    "]\n",
    "\n",
    "labels = ['Class1', 'Class2', 'Class1', 'Class2' ]  # Corresponding labels for each register\n",
    "\n",
    "# Encode registers as one-hot vectors\n",
    "def encode_register(register):\n",
    "    one_hot_vector = np.zeros(len(unique_properties))\n",
    "    for prop in register:\n",
    "        if prop in property_to_index:\n",
    "            one_hot_vector[property_to_index[prop]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Encode labels as one-hot vectors\n",
    "def encode_label(label, unique_labels):\n",
    "    label_vector = np.zeros(len(unique_labels))\n",
    "    label_index = unique_labels.index(label)\n",
    "    label_vector[label_index] = 1\n",
    "    return label_vector\n",
    "\n",
    "# Define the unique labels and create a vocabulary for labels\n",
    "unique_labels = list(set(labels))\n",
    "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# Encode the sample registers and labels\n",
    "encoded_registers = np.array([encode_register(register) for register in registers])\n",
    "encoded_labels = np.array([encode_label(label, unique_labels) for label in labels])\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an LSTM layer with input shape matching the number of features (properties)\n",
    "model.add(LSTM(units=64, input_shape=(len(unique_properties), 1)))  # Assuming one timestep per feature\n",
    "\n",
    "# Add a Dense layer with units equal to the number of unique labels\n",
    "num_classes = len(unique_labels)\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model using the encoded data and labels\n",
    "model.fit(encoded_registers[:, :, np.newaxis], encoded_labels, epochs=10)  # Add an extra axis for timestep\n",
    "\n",
    "# Make predictions for each register\n",
    "predictions = model.predict(encoded_registers[:, :, np.newaxis])\n",
    "print(\"Predictions for Registers:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FORMA</th>\n",
       "      <th>INF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>salegar</td>\n",
       "      <td>salegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salego</td>\n",
       "      <td>salegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salegas</td>\n",
       "      <td>salegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>salega</td>\n",
       "      <td>salegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>salegamos</td>\n",
       "      <td>salegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>salegais</td>\n",
       "      <td>salegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>salegan</td>\n",
       "      <td>salegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>he salegado</td>\n",
       "      <td>salegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>has salegado</td>\n",
       "      <td>salegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ha salegado</td>\n",
       "      <td>salegar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          FORMA      INF\n",
       "0       salegar  salegar\n",
       "1        salego  salegar\n",
       "2       salegas  salegar\n",
       "3        salega  salegar\n",
       "4     salegamos  salegar\n",
       "5      salegais  salegar\n",
       "6       salegan  salegar\n",
       "7   he salegado  salegar\n",
       "8  has salegado  salegar\n",
       "9   ha salegado  salegar"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebooks.functions.verbs_dictionary import *\n",
    "verbo = \"salegar\"\n",
    "conjugations = scrape_verb_conjugations(verbo)\n",
    "conjugations = move_inf_first_column(conjugations)\n",
    "conjugations = cast_vebs_df(conjugations)\n",
    "conjugations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abolir\n",
      "abolimos\n",
      "abolis\n",
      "he abolido\n",
      "has abolido\n",
      "ha abolido\n",
      "hemos abolido\n",
      "habeis abolido\n",
      "han abolido\n",
      "abolia\n",
      "abolias\n",
      "abolia\n",
      "aboliamos\n",
      "aboliais\n",
      "abolian\n",
      "habia abolido\n",
      "habias abolido\n",
      "habia abolido\n",
      "habiamos abolido\n",
      "habiais abolido\n",
      "habian abolido\n",
      "aboli\n",
      "aboliste\n",
      "abolio\n",
      "abolimos\n",
      "abolisteis\n",
      "abolieron\n",
      "hube abolido\n",
      "hubiste abolido\n",
      "hubo abolido\n",
      "hubimos abolido\n",
      "hubisteis abolido\n",
      "hubieron abolido\n",
      "abolire\n",
      "aboliras\n",
      "abolira\n",
      "aboliremos\n",
      "abolireis\n",
      "aboliran\n",
      "habre abolido\n",
      "habras abolido\n",
      "habra abolido\n",
      "habremos abolido\n",
      "habreis abolido\n",
      "habran abolido\n",
      "aboliria\n",
      "abolirias\n",
      "aboliria\n",
      "aboliriamos\n",
      "aboliriais\n",
      "abolirian\n",
      "habria abolido\n",
      "habrias abolido\n",
      "habria abolido\n",
      "habriamos abolido\n",
      "habriais abolido\n",
      "habrian abolido\n",
      "haya abolido\n",
      "hayas abolido\n",
      "haya abolido\n",
      "hayamos abolido\n",
      "hayais abolido\n",
      "hayan abolido\n",
      "aboliera\n",
      "abolieras\n",
      "aboliera\n",
      "abolieramos\n",
      "abolierais\n",
      "abolieran\n",
      "hubiera abolido\n",
      "hubieras abolido\n",
      "hubiera abolido\n",
      "hubieramos abolido\n",
      "hubierais abolido\n",
      "hubieran abolido\n",
      "aboliese\n",
      "abolieses\n",
      "aboliese\n",
      "aboliesemos\n",
      "abolieseis\n",
      "aboliesen\n",
      "hubiese abolido\n",
      "hubieses abolido\n",
      "hubiese abolido\n",
      "hubiesemos abolido\n",
      "hubieseis abolido\n",
      "hubiesen abolido\n",
      "aboliere\n",
      "abolieres\n",
      "aboliere\n",
      "abolieremos\n",
      "aboliereis\n",
      "abolieren\n",
      "hubiere abolido\n",
      "hubieres abolido\n",
      "hubiere abolido\n",
      "hubieremos abolido\n",
      "hubiereis abolido\n",
      "hubieren abolido\n",
      "abolid\n",
      "haber abolido\n",
      "aboliendo\n",
      "habiendo abolido\n",
      "abolido\n"
     ]
    }
   ],
   "source": [
    "# Print all the values of conjugations in the colum \"FORMA\"\n",
    "for i in range(len(conjugations)):\n",
    "    if conjugations[\"FORMA\"][i] == \"\" or len(conjugations[\"FORMA\"][i]) == 0:\n",
    "        continue\n",
    "    print(conjugations[\"FORMA\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(conjugations['FORMA'].iloc[-5] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, None, 64)          384       \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33538 (131.01 KB)\n",
      "Trainable params: 33538 (131.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'int'>\"}), <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/NB_try.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/NB_try.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m])  \u001b[39m# Corresponding labels for the samples\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/NB_try.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Train the model (replace with your own training data)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/NB_try.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(samples_ids_flat, labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/NB_try.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# Example test data with varying sequence lengths\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/NB_try.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m test_samples \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/NB_try.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     [\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/NB_try.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     [\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mE\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/NB_try.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/.venv/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/.venv/lib64/python3.9/site-packages/keras/src/engine/data_adapter.py:1105\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1102\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m   1103\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m   1104\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1106\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1107\u001b[0m             _type_name(x), _type_name(y)\n\u001b[1;32m   1108\u001b[0m         )\n\u001b[1;32m   1109\u001b[0m     )\n\u001b[1;32m   1110\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1111\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1112\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1113\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[1;32m   1115\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'int'>\"}), <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Sample data (replace with your own data)\n",
    "samples = [\n",
    "    ['A', 'B', 'C'],\n",
    "    ['B', 'D', 'E', 'F'],\n",
    "    ['A', 'C', 'F'],\n",
    "    # Add more samples with varying features\n",
    "]\n",
    "\n",
    "# Create a vocabulary of unique features across all samples\n",
    "vocab = set(feature for sample in samples for feature in sample)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Map each feature to an integer ID\n",
    "feature_to_id = {feature: idx for idx, feature in enumerate(vocab)}\n",
    "\n",
    "# Convert the samples to integer IDs using the vocabulary\n",
    "samples_ids = [[feature_to_id[feature] for feature in sample] for sample in samples]\n",
    "\n",
    "# Flatten the samples_ids list\n",
    "samples_ids_flat = [item for sublist in samples_ids for item in sublist]\n",
    "\n",
    "# Define the embedding dimension (you can experiment with different values)\n",
    "embedding_dim = 64\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Modify the input layer to accept variable-length sequences\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=None))\n",
    "\n",
    "# Add an LSTM layer (or other layers as needed)\n",
    "model.add(LSTM(64))\n",
    "\n",
    "# Add a Dense layer for classification\n",
    "num_classes = 2  # Replace with the number of classes in your problem\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Example labels (replace with your own labels)\n",
    "labels = np.array([0, 1, 0])  # Corresponding labels for the samples\n",
    "\n",
    "# Train the model (replace with your own training data)\n",
    "model.fit(samples_ids_flat, labels, epochs=10)\n",
    "\n",
    "# Example test data with varying sequence lengths\n",
    "test_samples = [\n",
    "    ['C', 'D'],\n",
    "    ['A', 'B', 'C', 'D', 'E'],\n",
    "]\n",
    "\n",
    "# Convert the test data to integer IDs using the vocabulary\n",
    "test_samples_ids = [[feature_to_id.get(feature, -1) for feature in sample] for sample in test_samples]\n",
    "\n",
    "# Pad the test data sequences to have the same length as the longest sequence in training data\n",
    "max_sequence_length = max(len(sample) for sample in samples)\n",
    "padded_test_samples_ids = tf.keras.preprocessing.sequence.pad_sequences(test_samples_ids, maxlen=max_sequence_length)\n",
    "\n",
    "# Make predictions for test samples\n",
    "predictions = model.predict(padded_test_samples_ids)\n",
    "print(\"Predictions for Test Samples:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
