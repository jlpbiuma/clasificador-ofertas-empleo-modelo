{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.functions.tools import load_json\n",
    "import pandas as pd\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import test set and dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11123, 5)\n"
     ]
    }
   ],
   "source": [
    "data = load_json('./data/test/test_palabras.json')\n",
    "# Now cast data to a DataFrame\n",
    "test_df = pd.DataFrame(data)\n",
    "print(test_df.shape)\n",
    "test_df.head()\n",
    "sustantives_df = pd.read_csv('./data/diccionario/df_structured_sustantivos.csv')\n",
    "adjectives_df = pd.read_csv('./data/diccionario/df_structured_adjetivos.csv')\n",
    "adverbs_df = pd.read_csv('./data/diccionario/df_structured_adverbios.csv')\n",
    "verbs_df = pd.read_csv('./data/diccionario/df_structured_verbos.csv')\n",
    "# Read the txt files sustantives and adjectives\n",
    "with open('./data/diccionario/list_unstructured_sustantivos.txt', 'r') as f:\n",
    "    sustantives_forms = f.read().splitlines()\n",
    "with open('./data/diccionario/list_unstructured_adjetivos.txt', 'r') as f:\n",
    "    adjectives_forms = f.read().splitlines()\n",
    "# with open('./data/diccionario/list_unstructured_verbs.txt', 'r') as f:\n",
    "#     verbs_forms = f.read().splitlines()\n",
    "with open('./data/diccionario/list_unstructured_adverbios.txt', 'r') as f:\n",
    "    adverbs_forms = f.read().splitlines()\n",
    "# Get the spanish stopwords with nltk\n",
    "stop_words = stopwords.words('spanish')\n",
    "# Cast all word to unicode\n",
    "sustantives_forms = [unidecode(word) for word in sustantives_forms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix verbs df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  (1021406, 2)\n",
      "After:  (554143, 2)\n"
     ]
    }
   ],
   "source": [
    "# Delete all samples that a one of the columns is null in verbs_df\n",
    "print(\"Before: \", verbs_df.shape)\n",
    "verbs_df = verbs_df.dropna()\n",
    "# Find all the verbs that have a space in the column \"FORMA\" and delete them\n",
    "verbs_df = verbs_df[verbs_df['FORMA'].str.contains(' ') == False]\n",
    "print(\"After: \", verbs_df.shape)\n",
    "verbs_df = verbs_df.sort_values(by=['FORMA']).reset_index(drop=True)\n",
    "# Get the list of verbs in the column \"FORMA\"\n",
    "verbs_forms = verbs_df['FORMA'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hacer\n"
     ]
    }
   ],
   "source": [
    "# Get the index of the verbs that are in the list of verbs_forms\n",
    "verb = \"hara\"\n",
    "index = verbs_forms.index(verb)\n",
    "# Get the INF value of the verb\n",
    "inf = verbs_df.iloc[index]['INF']\n",
    "print(inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "list_remove = [\"www\", \"com\",\"http\", \"https\"]\n",
    "\n",
    "def tokenize_descripcion(text):\n",
    "    # Remove links (URLs) from the text using regular expressions\n",
    "    text = re.sub(r'http(s)?:\\s+\\S+', '', text, flags=re.IGNORECASE)\n",
    "    # Remove all occurrences of \".es\" (case-insensitive)\n",
    "    text = re.sub(r'\\.es', '', text, flags=re.IGNORECASE)\n",
    "    # Remove all non alpha characters from the text using regular expressions\n",
    "    text = re.sub(r'[^a-zA-Z ]+', ' ', text, flags=re.IGNORECASE)\n",
    "    # Remove unnecessary spaces from the text using regular expressions\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.IGNORECASE)\n",
    "    # Cast all words to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def create_palabras_column(text):\n",
    "    # Split the text into a list of words and filter simultaneously\n",
    "    palabras = [palabra for palabra in text.split(\" \") if len(palabra) > 1 and palabra not in list_remove]\n",
    "    return palabras\n",
    "\n",
    "def list_words(palabras_empleo_texto):\n",
    "    # print(palabras_empleo_texto)\n",
    "    return palabras_empleo_texto.lower().split(\" \")[:-1]\n",
    "\n",
    "def clean_descripcion(df):\n",
    "    # tokenize the descripcion\n",
    "    df['descripcion_oferta'] = df['descripcion_oferta'].apply(tokenize_descripcion)\n",
    "    # Split the text into a list of words\n",
    "    df['palabras_descripcion_oferta'] = df['descripcion_oferta'].apply(create_palabras_column)\n",
    "    # Convert the string into a list\n",
    "    df['palabras_empleo_texto'] = df['palabras_empleo_texto'].apply(lambda x: list_words(x))\n",
    "    return df\n",
    "\n",
    "test_df = clean_descripcion(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexialice unstructured lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zu', 'zz', 'zu']\n"
     ]
    }
   ],
   "source": [
    "index_chars = 2\n",
    "\n",
    "def create_index_list(unstructured_forms):\n",
    "    index_list = {}\n",
    "    for index, string in enumerate(unstructured_forms):\n",
    "        first_letter = string[0:index_chars]\n",
    "        if first_letter not in index_list:\n",
    "            index_list[first_letter] = index\n",
    "    return index_list\n",
    "\n",
    "def find_indices(string, index_list, unstructured_forms):\n",
    "    if not string:  # Check if the string is empty\n",
    "        return 0, len(unstructured_forms)\n",
    "    first_letter = string[0:index_chars]\n",
    "    start_index = index_list.get(first_letter, 0)\n",
    "    if first_letter in limit_letters:\n",
    "        end_index = len(unstructured_forms)\n",
    "    else:\n",
    "        next_letter = get_next_letter(first_letter, index_list)\n",
    "        end_index = index_list.get(next_letter, len(unstructured_forms))\n",
    "    return start_index, end_index\n",
    "\n",
    "def get_next_letter(first_letter, index_list):\n",
    "    keys_list = list(index_list.keys())\n",
    "    if first_letter in keys_list:\n",
    "        next_letter_index_key = keys_list.index(first_letter) + 1\n",
    "        return keys_list[next_letter_index_key]\n",
    "    else:\n",
    "        return chr(ord(first_letter[0]) + 1)\n",
    "\n",
    "sustantives_index_list = create_index_list(sustantives_forms)\n",
    "adjectives_index_list = create_index_list(adjectives_forms)\n",
    "verbs_index_list = create_index_list(verbs_forms)\n",
    "# Get the first two letters of the column FORMA on each DataFrame\n",
    "limit_letters = [verbs_df['FORMA'].str[0:index_chars].iloc[-1],\n",
    "                 sustantives_df['FORMA'].str[0:index_chars].iloc[-1],\n",
    "                 adjectives_df['FORMA'].str[0:index_chars].iloc[-1]]\n",
    "print(limit_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    # Delete all numbers\n",
    "    word = re.sub(r\"\\d+\", \"\", word)\n",
    "    # Delete all accents\n",
    "    word = unidecode(word.lower())\n",
    "    # Remove simbols\n",
    "    word = re.sub(r\"[^a-z0-9ñ]\", \"\", word)\n",
    "    return word\n",
    "\n",
    "def get_reference(descripcion_oferta, df, index, column):\n",
    "    # Get the accuracy of the words in palabras_empleo_texto that are in descripcion_oferta\n",
    "    error = 0\n",
    "    accuracy = 0\n",
    "    if column == \"palabras_empleo_texto\":\n",
    "        list_words = df[column].iloc[index]\n",
    "    else:\n",
    "        list_words = df[column].iloc[index].lower().split(\" \")\n",
    "    \n",
    "    for word in descripcion_oferta.split(\" \"):\n",
    "        if unidecode(word) not in list_words:\n",
    "            error += 1\n",
    "        else:\n",
    "            accuracy += 1\n",
    "    return error, accuracy\n",
    "\n",
    "def add_words_to_vocabulary(palabras_list, vocabulary):\n",
    "    # Add to vocabulary all different words\n",
    "    for palabra in palabras_list:\n",
    "        if palabra not in vocabulary.keys():\n",
    "            vocabulary[palabra] = 1\n",
    "        else:\n",
    "            vocabulary[palabra] += 1\n",
    "    # Return the vocabulary\n",
    "    return vocabulary\n",
    "\n",
    "def show_process_stats(end_time_process, times, palabras, metrics, vocabulary):\n",
    "    # Print total of offers\n",
    "    print(\"Total of offers: {:.0f}\".format(len(times)))\n",
    "    # Print the time process\n",
    "    print(\"Time process: {:.2f} seg\".format(end_time_process))\n",
    "    # Get mean of times\n",
    "    mean_time = sum(times) / len(times)\n",
    "    print(\"Mean time: {:.2f} seg\".format(mean_time))\n",
    "    # Sum all the words is a list of lists\n",
    "    total_words = sum(palabras)\n",
    "    print(\"Total words: {:.0f}\".format(total_words))\n",
    "    # Get words per time\n",
    "    words_per_time = total_words / end_time_process\n",
    "    print(\"Words per time: {:.2f}\".format(words_per_time))\n",
    "    # Total number of different words\n",
    "    total_different_words = len(vocabulary.keys())\n",
    "    print(\"Total different words: {:.0f}\".format(total_different_words))\n",
    "    # Get the mean of er_nuevas, acc_nuevas, er_legacy, acc_legacy\n",
    "    er_nuevas = sum([metric[\"er_nuevas\"] for metric in metrics]) / len(metrics)\n",
    "    acc_nuevas = sum([metric[\"acc_nuevas\"] for metric in metrics]) / len(metrics)\n",
    "    er_legacy = sum([metric[\"er_legacy\"] for metric in metrics]) / len(metrics)\n",
    "    acc_legacy = sum([metric[\"acc_legacy\"] for metric in metrics]) / len(metrics)\n",
    "    # Print the metrics\n",
    "    print(\"Error medio producido en palabras nuevas: {:.2f}\".format(er_nuevas))\n",
    "    print(\"Precisión media obtenida en palabras nuevas: {:.2f}\".format(acc_nuevas))\n",
    "    print(\"Error medio anterio: {:.2f}\".format(er_legacy))\n",
    "    print(\"Precisión media anterior: {:.2f}\".format(acc_legacy))\n",
    "\n",
    "def get_lemma_df(word, df, unstructured_forms, index_list):\n",
    "    # start, end = (0, len(unstructured_forms))\n",
    "    start, end = find_indices(word, index_list, unstructured_forms)\n",
    "    # Is faster to handler the exception than to check if the word is in the list\n",
    "    try:\n",
    "        index = unstructured_forms.index(word, start, end)\n",
    "        if \"LEMA\" in df.columns:\n",
    "            return df.iloc[index][\"LEMA\"]\n",
    "        else:\n",
    "            return df.iloc[index][\"INF\"]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def conditional_inference(word, df_nouns, unstructured_forms_nouns, df_adj, unstructured_forms_adj, df_verbs, unstructured_forms_verb):\n",
    "    verb = get_lemma_df(word, df_verbs, unstructured_forms_verb, verbs_index_list)\n",
    "    if verb == None:\n",
    "        noun = get_lemma_df(word, df_nouns, unstructured_forms_nouns, sustantives_index_list)\n",
    "        if noun == None:\n",
    "            adjective = get_lemma_df(word, df_adj, unstructured_forms_adj, adjectives_index_list)\n",
    "            if adjective == None:\n",
    "                return None\n",
    "            else:\n",
    "                return adjective\n",
    "        else:\n",
    "            return noun\n",
    "    else:\n",
    "        return verb\n",
    "\n",
    "def get_syntax(words, df_nouns, unstructured_forms_nouns, df_adj, unstructured_forms_adj, unstructured_forms_adv, df_verbs, unstructured_forms_verb):\n",
    "    # Object palabras_list\n",
    "    palabras_list = []\n",
    "    # Split by \" \"\n",
    "    for word in words.split(\" \"):\n",
    "        word = clean_word(word)\n",
    "        if word in stop_words or word in unstructured_forms_adv:\n",
    "            continue\n",
    "        # Verify if is a noun\n",
    "        solution = conditional_inference(word, df_nouns, unstructured_forms_nouns, df_adj, unstructured_forms_adj, df_verbs, unstructured_forms_verb)\n",
    "        if solution != None:\n",
    "            palabras_list.append(solution)\n",
    "    return palabras_list\n",
    "\n",
    "def process_and_update_df(df, sustantives_df, sustantives_forms, adjectives_df, adjectives_forms, adverbs_forms ,verbs_df, verbs_forms):\n",
    "    import time\n",
    "    # Create a new column in the DataFrame to store the result\n",
    "    df[\"palabras_empleo_texto_nuevas\"] = \"\"\n",
    "    times = []\n",
    "    n_palabras = []\n",
    "    metrics = []\n",
    "    vocabulary = {}\n",
    "    start_time_process = time.time()\n",
    "    # Iterate over the DataFrame and apply the word extraction function\n",
    "    for index, description in df[\"descripcion_oferta\"].items():\n",
    "        start_time = time.time()\n",
    "        # Get the list of words\n",
    "        palabras_list = get_syntax(description, sustantives_df, sustantives_forms, adjectives_df, adjectives_forms, adverbs_forms ,verbs_df, verbs_forms)\n",
    "        # Add the words to the vocabulary\n",
    "        vocabulary = add_words_to_vocabulary(palabras_list, vocabulary)\n",
    "        # Save palabras_list in a new column in the DataFrame, insert the full list\n",
    "        df.at[index, \"palabras_empleo_texto_nuevas\"] = ' '.join(palabras_list).upper()\n",
    "        # Calculate partial time and save to times list\n",
    "        times.append(time.time() - start_time)\n",
    "        # Save the number of words in the list\n",
    "        n_palabras.append(len(palabras_list))\n",
    "        # Get the metrics of the calculated words\n",
    "        er_nuevas, acc_nuevas = get_reference(description, df, index, \"palabras_empleo_texto_nuevas\")\n",
    "        # Get the metrics of the legacy words\n",
    "        er_legacy, acc_legacy = get_reference(description, df, index, \"palabras_empleo_texto\")\n",
    "        # Add to metrics list\n",
    "        metrics.append({\"er_nuevas\": er_nuevas, \"acc_nuevas\": acc_nuevas, \"er_legacy\": er_legacy, \"acc_legacy\": acc_legacy})\n",
    "    # Calculate and add the column with words that appear in palabras legacy but not in palabras nuevas\n",
    "    df[\"palabras_legacy_minus_nuevas\"] = df.apply(lambda row: list(set(row[\"palabras_empleo_texto\"]) - set(row[\"palabras_empleo_texto_nuevas\"])), axis=1)\n",
    "    end_time_process = time.time() - start_time_process\n",
    "    show_process_stats(end_time_process, times, n_palabras, metrics, vocabulary)\n",
    "    return df, times, n_palabras, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of offers: 100\n",
      "Time process: 6.54 seg\n",
      "Mean time: 0.06 seg\n",
      "Total words: 7248\n",
      "Words per time: 1109.05\n",
      "Error medio producido en palabras nuevas: 101.28\n",
      "Precisión media obtenida en palabras nuevas: 38.93\n",
      "Error medio anterio: 123.75\n",
      "Precisión media anterior: 16.46\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_puesto_esco_ull</th>\n",
       "      <th>categoria</th>\n",
       "      <th>subcategoria</th>\n",
       "      <th>palabras_empleo_texto</th>\n",
       "      <th>descripcion_oferta</th>\n",
       "      <th>palabras_descripcion_oferta</th>\n",
       "      <th>palabras_empleo_texto_nuevas</th>\n",
       "      <th>palabras_legacy_minus_nuevas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1634</td>\n",
       "      <td>Atencion a clientes</td>\n",
       "      <td>Atencion al cliente</td>\n",
       "      <td>[administrativo, persona, reservas, buceo, ges...</td>\n",
       "      <td>buscamos una persona encargada de gestionar la...</td>\n",
       "      <td>[buscamos, una, persona, encargada, de, gestio...</td>\n",
       "      <td>BUSCAR PERSONA ENCARGADO GESTIONAR RESERVAR AC...</td>\n",
       "      <td>[persona, data, fisica, gestion, ventas, trami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984</td>\n",
       "      <td>Ventas al detalle</td>\n",
       "      <td>Venta al detalle</td>\n",
       "      <td>[dependiente, tiendas, centro, comercial, expe...</td>\n",
       "      <td>se busca dependiente para la tienda tezenis en...</td>\n",
       "      <td>[se, busca, dependiente, para, la, tienda, tez...</td>\n",
       "      <td>BUSCAR DEPENDIENTE TENDER CENTRAR COMERCIAL ME...</td>\n",
       "      <td>[idiomas, tiendas, dependiente, comercial, cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>719</td>\n",
       "      <td>Recursos humanos</td>\n",
       "      <td>Prevencion de riesgos</td>\n",
       "      <td>[puentes, grua, normas, sector, metales, homol...</td>\n",
       "      <td>grupo loxamhune empresa lider en el alquiler d...</td>\n",
       "      <td>[grupo, loxamhune, empresa, lider, en, el, alq...</td>\n",
       "      <td>EMPRESA LIDER ALQUILER MAQUINAR PLATAFORMA ELE...</td>\n",
       "      <td>[freelance, pemp, psicosociologia, ergonomia, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1508</td>\n",
       "      <td>Comercial y ventas</td>\n",
       "      <td>Comercial</td>\n",
       "      <td>[asesores, comercial, prevencion, riesgos, lab...</td>\n",
       "      <td>antea prevencion es una compania dedicada a la...</td>\n",
       "      <td>[antea, prevencion, es, una, compania, dedicad...</td>\n",
       "      <td>ANTEA PREVENCION COMPANIA DEDICADO PREVENCION ...</td>\n",
       "      <td>[asesorias, persona, crm, seguros, prospeccion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2280</td>\n",
       "      <td>Ingenieros y tecnicos</td>\n",
       "      <td>Electronica y automatica industrial</td>\n",
       "      <td>[oficial, mantenimiento, electromecanico, agua...</td>\n",
       "      <td>mantenimiento preventivo y correctivo de sist...</td>\n",
       "      <td>[mantenimiento, preventivo, correctivo, de, si...</td>\n",
       "      <td>MANTENIMIENTO PREVENTIVO CORRECTIVO SISTEMA EL...</td>\n",
       "      <td>[preventivo, plantas, equipos, gestion, oficia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_puesto_esco_ull              categoria  \\\n",
       "0                1634    Atencion a clientes   \n",
       "1                1984      Ventas al detalle   \n",
       "2                 719       Recursos humanos   \n",
       "3                1508     Comercial y ventas   \n",
       "4                2280  Ingenieros y tecnicos   \n",
       "\n",
       "                          subcategoria  \\\n",
       "0                  Atencion al cliente   \n",
       "1                     Venta al detalle   \n",
       "2                Prevencion de riesgos   \n",
       "3                            Comercial   \n",
       "4  Electronica y automatica industrial   \n",
       "\n",
       "                               palabras_empleo_texto  \\\n",
       "0  [administrativo, persona, reservas, buceo, ges...   \n",
       "1  [dependiente, tiendas, centro, comercial, expe...   \n",
       "2  [puentes, grua, normas, sector, metales, homol...   \n",
       "3  [asesores, comercial, prevencion, riesgos, lab...   \n",
       "4  [oficial, mantenimiento, electromecanico, agua...   \n",
       "\n",
       "                                  descripcion_oferta  \\\n",
       "0  buscamos una persona encargada de gestionar la...   \n",
       "1  se busca dependiente para la tienda tezenis en...   \n",
       "2  grupo loxamhune empresa lider en el alquiler d...   \n",
       "3  antea prevencion es una compania dedicada a la...   \n",
       "4   mantenimiento preventivo y correctivo de sist...   \n",
       "\n",
       "                         palabras_descripcion_oferta  \\\n",
       "0  [buscamos, una, persona, encargada, de, gestio...   \n",
       "1  [se, busca, dependiente, para, la, tienda, tez...   \n",
       "2  [grupo, loxamhune, empresa, lider, en, el, alq...   \n",
       "3  [antea, prevencion, es, una, compania, dedicad...   \n",
       "4  [mantenimiento, preventivo, correctivo, de, si...   \n",
       "\n",
       "                        palabras_empleo_texto_nuevas  \\\n",
       "0  BUSCAR PERSONA ENCARGADO GESTIONAR RESERVAR AC...   \n",
       "1  BUSCAR DEPENDIENTE TENDER CENTRAR COMERCIAL ME...   \n",
       "2  EMPRESA LIDER ALQUILER MAQUINAR PLATAFORMA ELE...   \n",
       "3  ANTEA PREVENCION COMPANIA DEDICADO PREVENCION ...   \n",
       "4  MANTENIMIENTO PREVENTIVO CORRECTIVO SISTEMA EL...   \n",
       "\n",
       "                        palabras_legacy_minus_nuevas  \n",
       "0  [persona, data, fisica, gestion, ventas, trami...  \n",
       "1  [idiomas, tiendas, dependiente, comercial, cen...  \n",
       "2  [freelance, pemp, psicosociologia, ergonomia, ...  \n",
       "3  [asesorias, persona, crm, seguros, prospeccion...  \n",
       "4  [preventivo, plantas, equipos, gestion, oficia...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first 5 rows of the DataFrame and save into a new DataFrame\n",
    "partial_df = test_df.iloc[0:100].copy()\n",
    "# print(partial_df[\"descripcion_oferta\"].iloc[0])\n",
    "# Call the processing function with your DataFrame\n",
    "partial_df, time_list, n_palabras = process_and_update_df(partial_df, sustantives_df, sustantives_forms, adjectives_df, adjectives_forms, adverbs_forms ,verbs_df, verbs_forms)\n",
    "partial_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"./data/test/test_palabras_nuevas.csv\", index=False)\n",
    "\n",
    "\n",
    "def export_results_to_markdown(index, test_df, output_file, time):\n",
    "    # Get list of words in descripcion_oferta\n",
    "    descripcion_oferta = test_df[\"descripcion_oferta\"].iloc[index]\n",
    "    error, accuracy = get_reference(descripcion_oferta, test_df, index, \"palabras_empleo_texto_nuevas\")\n",
    "    \n",
    "    with open(\"./test/\" + str(index) + \"_\" + output_file, \"w\") as md_file:\n",
    "        # Write header for the section\n",
    "        md_file.write(f\"## Descripcion oferta: \\n{test_df['descripcion_oferta'].iloc[index]}\\n\")\n",
    "        md_file.write(f\"### Total de palabras en descripción: \\n{len(descripcion_oferta)}\\n\")\n",
    "        md_file.write(f\"\\n\")\n",
    "        md_file.write(f\"## Palabras nuevas: \\n{test_df['palabras_empleo_texto_nuevas'].iloc[index]}\\n\")\n",
    "        md_file.write(f\"### Accuracy - palabras nuevas: \\n{accuracy}\\n\")\n",
    "        md_file.write(f\"### Error - palabras nuevas: \\n{error}\\n\")\n",
    "        palabras_not_found = list(set(test_df['palabras_empleo_texto_nuevas'].iloc[index].lower().split(\" \")) - set(descripcion_oferta.split(\" \")))\n",
    "        md_file.write(f\"## Palabras nuevas no encontradas en descripción: \\n{', '.join(palabras_not_found)}\\n\")\n",
    "        md_file.write(f\"\\n\")\n",
    "        error, accuracy = get_reference(descripcion_oferta, test_df, index, \"palabras_empleo_texto\")\n",
    "        md_file.write(f\"## Palabras legacy: \\n{', '.join(test_df['palabras_empleo_texto'].iloc[index])}\\n\")\n",
    "        md_file.write(f\"### Accuracy - palabras legacy: \\n{accuracy}\\n\")\n",
    "        md_file.write(f\"### Error - palabras legacy: \\n{error}\\n\")\n",
    "        palabras_not_found = list(set(test_df['palabras_empleo_texto'].iloc[index]) - set(descripcion_oferta))\n",
    "        md_file.write(f\"## Palabras legacy no encontradas en descripción: \\n{', '.join(palabras_not_found)}\\n\")\n",
    "        md_file.write(f\"\\n\")\n",
    "        md_file.write(f\"## Tiempo de ejecución: \\n{time} segundos\\n\")\n",
    "        md_file.write(f\"## Relación palabras/tiempo: \\n{len(descripcion_oferta)/time} palabras/segundo\\n\")\n",
    "\n",
    "# Define the Markdown output file\n",
    "output_file = \"test.md\"\n",
    "\n",
    "# Call the function to export the results to Markdown\n",
    "for index in range(0, 5):\n",
    "    time = time_list[index]\n",
    "    export_results_to_markdown(index, partial_df, output_file, time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
