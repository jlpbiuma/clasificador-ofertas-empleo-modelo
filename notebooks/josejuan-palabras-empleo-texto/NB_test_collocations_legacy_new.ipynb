{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.collocations import ngrams\n",
    "import re\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\"\n",
    "dictionary_path = \"./static/\"\n",
    "stopwords = stopwords.words('spanish')\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "df_empleo = pd.read_csv(dictionary_path + \"diccionario_empleo.csv\")\n",
    "df_equivalencias = pd.read_csv(dictionary_path + \"diccionario_equivalencias.csv\")\n",
    "df_collocations = pd.read_csv(dictionary_path + \"diccionario_collocation.csv\")\n",
    "list_collocations = df_collocations[\"FORMAS\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy collacations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_vector_collocation_json(texto):\n",
    "\n",
    "    # adverbios\n",
    "    adverbios = ['ahora', 'antes', 'despues', 'tarde', 'luego', 'ayer', 'temprano', 'ya',\n",
    "                 'todavia', 'anteayer', 'aun', 'pronto', 'hoy', 'aqui', 'ahi', 'alli',\n",
    "                 'cerca', 'lejos', 'fuera', 'dentro', 'alrededor', 'aparte', 'encima',\n",
    "                 'debajo', 'delante', 'detras', 'asi', 'bien', 'mal', 'despacio',\n",
    "                 'deprisa', 'como', 'mucho', 'poco', 'muy', 'casi', 'todo', 'nada', 'algo',\n",
    "                 'medio', 'demasiado', 'bastante', 'mas', 'menos', 'ademas', 'incluso',\n",
    "                 'tambien', 'si', 'asimismo', 'no', 'tampoco', 'jamas', 'nunca', 'acaso',\n",
    "                 'quiza', 'quizas', 'tal', 'vez', 'mejor']\n",
    "    stopwords.extend(adverbios)\n",
    "\n",
    "    # obtengo en un dataframe el diccionario de collocation\n",
    "    with open(dictionary_path + 'diccionario_collocation.json',\n",
    "              encoding=\"latin-1\") as f:\n",
    "        aux_collocations = f.read()\n",
    "    diccionario_collocations = json.loads(aux_collocations)\n",
    "\n",
    "    # obtengo en un dataframe el diccionario de equivalencias de collocation\n",
    "    with open(dictionary_path + 'diccionario_equivalencias_collocation.json',\n",
    "              encoding=\"latin-1\") as f:\n",
    "        aux_equiv = f.read()\n",
    "    dic_equiv_collocations = json.loads(aux_equiv)\n",
    "\n",
    "    vector_collocation = list()\n",
    "    collocations_dic = list()\n",
    "    collocations_equiv = list()\n",
    "\n",
    "    for i in range(len(diccionario_collocations)):\n",
    "        collocations_dic.append(diccionario_collocations[i]['RAIZ_COLLOCATION'])\n",
    "\n",
    "    for i in range(len(dic_equiv_collocations)):\n",
    "        collocations_equiv.append(dic_equiv_collocations[i]['RAIZ_COLLOCATION'])\n",
    "\n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "    lista_palabras = str(texto).lower()\n",
    "    palabras = lista_palabras.split()\n",
    "    filtered_words = [word for word in palabras if word not in stopwords]\n",
    "\n",
    "    table = {33: 32, 35: 32, 36: 32, 37: 32, 94: 32, 38: 32, 42: 32, 40: 32, 41: 32, 91: 32, 93: 32,\n",
    "             123: 32, 125: 32, 58: 32, 59: 32, 44: 32, 47: 32, 60: 32, 62: 32, 92: 32, 124: 32, 96: 32,\n",
    "             126: 32, 45: 32, 34: 32, 39: 32, 61: 32, 95: 32, 43: 32}\n",
    "\n",
    "    palabras_encontradas = []\n",
    "\n",
    "    for j in range(len(filtered_words)):\n",
    "        sin_simbolo = filtered_words[j].replace(\"¡\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"»\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"«\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"¿\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"°\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"º\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ª\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"_x000d_\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"@\", \"o\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"á\", \"a\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"à\", \"a\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"À\", \"a\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Á\", \"a\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"é\", \"e\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"è\", \"e\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"È\", \"e\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"É\", \"e\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"í\", \"i\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ì\", \"i\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Ì\", \"i\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Í\", \"i\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ó\", \"o\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ò\", \"o\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Ò\", \"o\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Ó\", \"o\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ú\", \"u\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ù\", \"u\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Ù\", \"u\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Ú\", \"u\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ñ\", \"ñ\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"?\", \"\")\n",
    "        tratar = sin_simbolo.strip(r\"!#$%^&*()[]{};:,./<>?\\|`~-'=_·\")\n",
    "        tratar2 = tratar.translate(table)\n",
    "        palabras = tratar2.split()\n",
    "        tokens = []\n",
    "        for palabra in palabras:\n",
    "            if palabra not in stopwords:\n",
    "                tokens.append(palabra.upper())\n",
    "        for w in tokens:\n",
    "            es_numero = w.isdigit()\n",
    "            if len(w) > 2 and not es_numero:\n",
    "                palabras_encontradas.append(w)\n",
    "    collocations = []\n",
    "    for n in [2, 3]:\n",
    "        collocations.extend(nltk.ngrams(palabras_encontradas, n))\n",
    "\n",
    "    for coll in collocations:\n",
    "        if len(coll) == 2:\n",
    "            coll_final = coll[0] + \" \" + coll[1]\n",
    "        if len(coll) == 3:\n",
    "            coll_final = coll[0] + \" \" + coll[1] + \" \" + coll[2]\n",
    "        coll_sep = coll_final.split(sep=' ')\n",
    "        coll_def = \"\"\n",
    "        for k in range(len(coll_sep)):\n",
    "            coll_def = coll_def + stemmer.stem(coll_sep[k]).upper() + \" \"\n",
    "        coll_def = coll_def[:-1]\n",
    "        if coll_def in collocations_dic and coll_final not in vector_collocation:\n",
    "            vector_collocation.append(coll_final)\n",
    "        elif coll_def in collocations_equiv and coll_final not in vector_collocation:\n",
    "            vector_collocation.append(coll_final)\n",
    "\n",
    "    value = {\n",
    "        \"vector_collocation\": vector_collocation\n",
    "    }\n",
    "\n",
    "    return json.dumps(value)\n",
    "\n",
    "def f_vector_collocation_json(texto):\n",
    "\n",
    "    # adverbios\n",
    "    adverbios = ['ahora', 'antes', 'despues', 'tarde', 'luego', 'ayer', 'temprano', 'ya',\n",
    "                 'todavia', 'anteayer', 'aun', 'pronto', 'hoy', 'aqui', 'ahi', 'alli',\n",
    "                 'cerca', 'lejos', 'fuera', 'dentro', 'alrededor', 'aparte', 'encima',\n",
    "                 'debajo', 'delante', 'detras', 'asi', 'bien', 'mal', 'despacio',\n",
    "                 'deprisa', 'como', 'mucho', 'poco', 'muy', 'casi', 'todo', 'nada', 'algo',\n",
    "                 'medio', 'demasiado', 'bastante', 'mas', 'menos', 'ademas', 'incluso',\n",
    "                 'tambien', 'si', 'asimismo', 'no', 'tampoco', 'jamas', 'nunca', 'acaso',\n",
    "                 'quiza', 'quizas', 'tal', 'vez', 'mejor']\n",
    "    stopwords.extend(adverbios)\n",
    "\n",
    "    # obtengo en un dataframe el diccionario de collocation\n",
    "    with open(dictionary_path + 'diccionario_collocation.json',\n",
    "              encoding=\"latin-1\") as f:\n",
    "        aux_collocations = f.read()\n",
    "    diccionario_collocations = json.loads(aux_collocations)\n",
    "\n",
    "    # obtengo en un dataframe el diccionario de equivalencias de collocation\n",
    "    with open(dictionary_path + 'diccionario_equivalencias_collocation.json',\n",
    "              encoding=\"latin-1\") as f:\n",
    "        aux_equiv = f.read()\n",
    "    dic_equiv_collocations = json.loads(aux_equiv)\n",
    "\n",
    "    vector_collocation = list()\n",
    "    collocations_dic = list()\n",
    "    collocations_equiv = list()\n",
    "\n",
    "    for i in range(len(diccionario_collocations)):\n",
    "        collocations_dic.append(diccionario_collocations[i]['RAIZ_COLLOCATION'])\n",
    "\n",
    "    for i in range(len(dic_equiv_collocations)):\n",
    "        collocations_equiv.append(dic_equiv_collocations[i]['RAIZ_COLLOCATION'])\n",
    "\n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "    lista_palabras = str(texto).lower()\n",
    "    palabras = lista_palabras.split()\n",
    "    filtered_words = [word for word in palabras if word not in stopwords]\n",
    "\n",
    "    table = {33: 32, 35: 32, 36: 32, 37: 32, 94: 32, 38: 32, 42: 32, 40: 32, 41: 32, 91: 32, 93: 32,\n",
    "             123: 32, 125: 32, 58: 32, 59: 32, 44: 32, 47: 32, 60: 32, 62: 32, 92: 32, 124: 32, 96: 32,\n",
    "             126: 32, 45: 32, 34: 32, 39: 32, 61: 32, 95: 32, 43: 32}\n",
    "\n",
    "    palabras_encontradas = []\n",
    "\n",
    "    for j in range(len(filtered_words)):\n",
    "        sin_simbolo = filtered_words[j].replace(\"¡\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"»\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"«\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"¿\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"°\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"º\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ª\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"_x000d_\", \"\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"@\", \"o\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"á\", \"a\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"à\", \"a\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"À\", \"a\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Á\", \"a\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"é\", \"e\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"è\", \"e\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"È\", \"e\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"É\", \"e\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"í\", \"i\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ì\", \"i\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Ì\", \"i\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Í\", \"i\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ó\", \"o\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ò\", \"o\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Ò\", \"o\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Ó\", \"o\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ú\", \"u\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ù\", \"u\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Ù\", \"u\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"Ú\", \"u\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"ñ\", \"ñ\")\n",
    "        sin_simbolo = sin_simbolo.replace(\"?\", \"\")\n",
    "        tratar = sin_simbolo.strip(r\"!#$%^&*()[]{};:,./<>?\\|`~-'=_·\")\n",
    "        tratar2 = tratar.translate(table)\n",
    "        palabras = tratar2.split()\n",
    "        tokens = []\n",
    "        for palabra in palabras:\n",
    "            if palabra not in stopwords:\n",
    "                tokens.append(palabra.upper())\n",
    "        for w in tokens:\n",
    "            es_numero = w.isdigit()\n",
    "            if len(w) > 2 and not es_numero:\n",
    "                palabras_encontradas.append(w)\n",
    "    collocations = []\n",
    "    for n in [2, 3]:\n",
    "        collocations.extend(nltk.ngrams(palabras_encontradas, n))\n",
    "\n",
    "    for coll in collocations:\n",
    "        if len(coll) == 2:\n",
    "            coll_final = coll[0] + \" \" + coll[1]\n",
    "        if len(coll) == 3:\n",
    "            coll_final = coll[0] + \" \" + coll[1] + \" \" + coll[2]\n",
    "        coll_sep = coll_final.split(sep=' ')\n",
    "        coll_def = \"\"\n",
    "        for k in range(len(coll_sep)):\n",
    "            coll_def = coll_def + stemmer.stem(coll_sep[k]).upper() + \" \"\n",
    "        coll_def = coll_def[:-1]\n",
    "        if coll_def in collocations_dic and coll_final not in vector_collocation:\n",
    "            vector_collocation.append(coll_final)\n",
    "        elif coll_def in collocations_equiv and coll_final not in vector_collocation:\n",
    "            vector_collocation.append(coll_final)\n",
    "\n",
    "    value = {\n",
    "        \"vector_collocation\": vector_collocation\n",
    "    }\n",
    "\n",
    "    return json.dumps(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to upper case all stopwords\n",
    "stopwords = [word.upper() for word in stopwords]\n",
    "\n",
    "def clean_text(text):\n",
    "    # ! EXCEPCIONES MANUALES!!!\n",
    "    # Cast exceptions to \"a\"\n",
    "    text = re.sub(r'@', 'a', text)\n",
    "    # Cast from \".\" to \"\"\n",
    "    text = re.sub(r'\\.', '', text)\n",
    "    # Trim the text\n",
    "    text = text.strip()\n",
    "    # Remove accents using unidecode, excluding 'ñ' and 'Ñ'\n",
    "    text = ''.join(char if char in ('ñ', 'Ñ') else unidecode(char) for char in text)\n",
    "    # Delete non-alphanumeric characters\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Delete additional spaces with regex\n",
    "    text = re.sub(r'\\s+', ' ', text).upper()\n",
    "    return text\n",
    "\n",
    "def filter_words(text):\n",
    "    # Split the text by \" \"\n",
    "    list_words = text.split(\" \")\n",
    "    # Delete empty strings\n",
    "    list_words = list(filter(None, list_words))\n",
    "    # Filter all stopwords\n",
    "    return [word for word in list_words if word not in stopwords]\n",
    "\n",
    "def get_list_stems(list_words):\n",
    "    # Get stems\n",
    "    return [stemmer.stem(word).upper() for word in list_words]\n",
    "\n",
    "def get_n_gramas(list_stems, min_n_gramas, max_n_gramas):\n",
    "    list_n_gramas = [\" \".join(n_grama) for n in range(min_n_gramas, max_n_gramas + 1)\n",
    "                     for n_grama in ngrams(list_stems, n)]\n",
    "    return list_n_gramas\n",
    "\n",
    "def calculate_forms(text, min_n_gramas=2, max_n_gramas=4):\n",
    "    # First clean text\n",
    "    text = clean_text(text)\n",
    "    # Filter words\n",
    "    list_words = filter_words(text)\n",
    "    # Get list of stems\n",
    "    list_stems = get_list_stems(list_words)\n",
    "    # Get the n gramas\n",
    "    list_n_gramas = get_n_gramas(list_stems, min_n_gramas, max_n_gramas)\n",
    "    if list_n_gramas != []:\n",
    "        return list_n_gramas\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_collocations(descripcion_oferta):\n",
    "    # Get all forms from the description\n",
    "    list_forms = calculate_forms(descripcion_oferta)\n",
    "    # Find in the list of collocations\n",
    "    list_collocations_found = [form for form in list_forms if form in list_collocations]\n",
    "    # Get the LEMA from the collocations\n",
    "    list_collocations_found = [df_collocations[df_collocations[\"FORMAS\"] == form][\"LEMA\"].iloc[0] for form in list_collocations_found]\n",
    "    return list_collocations_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Get 100 random samples from the dataset\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m df_test \u001b[39m=\u001b[39m df_offers\u001b[39m.\u001b[39msample(\u001b[39m100\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m df_test[\u001b[39m\"\u001b[39m\u001b[39mPALABRAS_LEGACY\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_test[\u001b[39m\"\u001b[39;49m\u001b[39mdescripcion_oferta\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: json\u001b[39m.\u001b[39;49mloads(f_vector_palabras_json(x))[\u001b[39m\"\u001b[39;49m\u001b[39mvector_palabras\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m df_test[\u001b[39m\"\u001b[39m\u001b[39mCOLLOCATIONS_LEGACY\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_test[\u001b[39m\"\u001b[39m\u001b[39mdescripcion_oferta\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: json\u001b[39m.\u001b[39mloads(f_vector_collocation_json(x))[\u001b[39m\"\u001b[39m\u001b[39mvector_collocation\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m df_test[\u001b[39m\"\u001b[39m\u001b[39mPALABRAS_NEW\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_test[\u001b[39m\"\u001b[39m\u001b[39mdescripcion_oferta\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(get_words_by_text)\n",
      "File \u001b[0;32m~/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/.venv/lib64/python3.9/site-packages/pandas/core/series.py:4758\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4756\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4758\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4759\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   4760\u001b[0m         func,\n\u001b[1;32m   4761\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[1;32m   4762\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[1;32m   4763\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   4764\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m   4765\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/.venv/lib64/python3.9/site-packages/pandas/core/apply.py:1201\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[1;32m   1200\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1201\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/.venv/lib64/python3.9/site-packages/pandas/core/apply.py:1281\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[1;32m   1282\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[1;32m   1283\u001b[0m )\n\u001b[1;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1286\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mReturning a DataFrame from Series.apply when the supplied function \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1288\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturns a Series is deprecated and will be removed in a future \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1292\u001b[0m     )  \u001b[39m# GH52116\u001b[39;00m\n",
      "File \u001b[0;32m~/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/.venv/lib64/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[0;32m~/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/.venv/lib64/python3.9/site-packages/pandas/core/algorithms.py:1812\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1810\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1812\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[1;32m   1813\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1815\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[1;32m   1816\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Get 100 random samples from the dataset\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m df_test \u001b[39m=\u001b[39m df_offers\u001b[39m.\u001b[39msample(\u001b[39m100\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m df_test[\u001b[39m\"\u001b[39m\u001b[39mPALABRAS_LEGACY\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_test[\u001b[39m\"\u001b[39m\u001b[39mdescripcion_oferta\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: json\u001b[39m.\u001b[39;49mloads(f_vector_palabras_json(x))[\u001b[39m\"\u001b[39m\u001b[39mvector_palabras\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m df_test[\u001b[39m\"\u001b[39m\u001b[39mCOLLOCATIONS_LEGACY\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_test[\u001b[39m\"\u001b[39m\u001b[39mdescripcion_oferta\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: json\u001b[39m.\u001b[39mloads(f_vector_collocation_json(x))[\u001b[39m\"\u001b[39m\u001b[39mvector_collocation\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.70.205/home/fulp/clasificador_ofertas_empleo/clasificador-ofertas-empleo-modelo/notebooks/josejuan-palabras-empleo-texto/NB_test_collocations_legacy_new.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m df_test[\u001b[39m\"\u001b[39m\u001b[39mPALABRAS_NEW\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_test[\u001b[39m\"\u001b[39m\u001b[39mdescripcion_oferta\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(get_words_by_text)\n",
      "File \u001b[0;32m/usr/lib64/python3.9/json/__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(s, (\u001b[39mbytes\u001b[39m, \u001b[39mbytearray\u001b[39m)):\n\u001b[0;32m--> 339\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    340\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not list"
     ]
    }
   ],
   "source": [
    "df_offers = pd.read_json(data_path + \"descripcion_ofertas_infojobs_21_23.json\")\n",
    "# Get 100 random samples from the dataset\n",
    "df_test = df_offers.sample(100)\n",
    "\n",
    "df_test[\"COLLOCATIONS_LEGACY\"] = df_test[\"descripcion_oferta\"].apply(lambda x: json.loads(f_vector_collocation_json(x))[\"vector_collocation\"])\n",
    "df_test[\"COLLOCATIONS_NEW\"] = df_test[\"descripcion_oferta\"].apply(get_collocations)\n",
    "\n",
    "# Save to json file\n",
    "df_test.to_json(data_path + \"test_collocations_legacy_new.json\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
